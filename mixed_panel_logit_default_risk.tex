\documentclass[12pt]{article}
\setcounter{secnumdepth}{2}

\usepackage{amssymb,amsmath,amsfonts,bm,enumitem,eurosym,geometry,ulem,graphicx,caption,color,relsize,setspace,sectsty,comment,footmisc,caption,natbib,pdflscape,subfigure,array,hyperref}

\normalem

\onehalfspacing
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}[hyp]
\renewcommand{\thesubhyp}{\thehyp\alph{subhyp}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\arraybackslash\hspace{0pt}}m{#1}}

\geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

\begin{document}

\begin{titlepage}
\title{Forecasting Sovereign External Debt Default \\ via Mixed Panel Logit Simulation}
\author{Kenneth Rios\footnotemark[1] \and Shukrit Guha\thanks{New York University, Department of Economics}}
\date{\today}
\maketitle
\begin{abstract}
\noindent We implement a panel data extension of a mixed logit model in order to forecast sovereign external debt defaults from 2010 to 2017. We fit a simulated version of the model on historical data using an assortment of lagged economic variables hypothesized to signal default and include year-fixed effects to control for year-specific influences. The marginal effects of the variables are allowed to be randomized across countries in order to reflect country-level heterogeneity in variable-specific influences. The model is simulated using Monte Carlo methods over parameterized independent normal distributions. Maximum likelihood estimation is penalized with an L2 regularization term to improve out-of-sample performance. The optimal tuning parameter is estimated using 10-fold cross-validation. The calibrated model is then used to predict unconditional probabilities of default using the test data. \\
\vspace{0in}\\
\noindent\textbf{Keywords:} sovereign default, mixed panel logit model, Monte Carlo simulation, L2 regularization \\
\vspace{0in}\\
\noindent\textbf{JEL Codes:} C530\\

\bigskip
\end{abstract}
\setcounter{page}{0}
\thispagestyle{empty}
\end{titlepage}
\pagebreak \newpage




\singlespacing


\section{Introduction} \label{sec:introduction}

Sovereign defaults are one of the main sources of international country risk. The ability to anticipate default events is a much sought after resource by foreign investors. To that extent, credit rating agencies have filled the void in the prediction market. Credit rating agencies, such as Moody's, Fitch, and S\&P

\section{Theory} \label{sec:theory}

\subsection{Background and Justification}

We model external debt defaults across countries as outcomes that are a function of the economic fundamentals of a given country as well as year-specific effects that may induce default. We proxy for countries' economic fundamentals using our most complete available assortment of economic variables that characterize a country's ``structural features", ``macroeconomic performance", ``public finances", and ``external finances". These four factors are considered by Moody's \textit{Sovereign Rating Model}, which purports to classify countries according to their estimated ability to repay debt obligations in full and on time. We lag these variables as is customary in much of the country risk literature (see ...). Lagging variables also allows us to implement fully forward-looking prediction, thus making the model of practical interest to forecasters. Finally, it is often the case that sovereign defaults are either underway or ``officially begin" near the start of a calender year. Since we do not have data of finer level of granularity than annual, such as quarterly data, using lags are a safeguard against defaults occurring very early in the year which would lead to end-of-year economic figures that are not of interest to the forecaster.

The year-specific effects we include may be interpreted as modeling contagion events within financial and foreign exchange markets that are internationally linked. Specifically, we expect year-fixed effects to be a reasonable approach to control for years of global recession that affected numerous countries, such as the global financial crisis of 2008.

There is an obvious distribution of the quality and characteristics of socioeconomic institutions across countries. Countries thus vary in their degree of default susceptibility on external debt according to the various predictors. For example, some countries have historically demonstrated to be particularly sensitive to changes in public debt (e.g., Venezuela), whereas other countries possess the financial stability to handle increasing levels of debt that countries like Venezuela may find unsustainable (e.g., United States). Thus, in order to address the likely heterogeneity of marginal effects of these variables across countries, we allow the vector of coefficients $\boldsymbol{\beta}_n$ to be randomized across countries, indexed by $n$. Allowing $\boldsymbol{\beta}$ to be randomized across countries not only accounts for this heterogeneity of marginal effects, but it also increases the flexibility of our model and allows us to simulate for predicted \textit{unconditional} default probabilities.
Our ultimate dependent variable of interest that we wish to predict is default status (=1 for a country defaulting in a given year, =0 otherwise). 

Models that incorporate randomization in addition to fixed effects are known as \textit{mixed models}. Since our ultimate dependent variable of interest is binary, we will be employing a \textit{mixed logit model}, whose distributional assumptions will be explained in subsection \ref{ssec:model}. Mixed logit models represent a generalization of standard logit models, such as logistic regression. Unlike standard logit models, mixed logit models allow for randomization of marginal effects and the correlation of unobserved influences over time. In some literature, mixed logit models are considered either equivalent to or a type of \textit{random coefficients} model, for obvious reasons.

Since we are interested in estimation over a panel dataset in order to model temporal influences, we introduce an extension to the mixed logit model that we call a \textit{mixed panel logit model}, in the same vein as Cherchi \& Cirillo (2008). This extension does not complicate the mathematics of the model substantially as we will assume that the noise terms present in the data-generating process are independent over time. Once closed-forms for the unconditional probabilities of default are derived via Monte Carlo simulation, we introduce L2 regularization that purports to improve the performance of our estimated model with the prediction (test) data. We decide on the optimal tuning parameter by implementing $k$-fold cross-validation. Finally, we fit the cross-validated model on the entirety of the training set and generate probability predictions using the test data. We wish to compare our model's prediction performance with that of credit rating agencies.

\subsection{Mixed Panel Logit Model of External Debt Default} \label{ssec:model}

For a given country $n$ in year $t$, the data-generating process decides whether the country ends up defaulting on its external debt obligations as $j = 1$ or not defaulting as $j = 0$ according to
\begin{align}
D_{njt} = \boldsymbol{\beta}_n^{'} \boldsymbol{x}_{njt-1} + \boldsymbol{\alpha}^{'}\boldsymbol{\gamma}_t + \varepsilon_{njt} \\ \nonumber
\\ 
\text{s.t.} \
d_{njt}=
\begin{cases}
1 & if D_{n1t} > D_{n0t} \nonumber \\ 
0 & \text{otherwise},
\end{cases}
\end{align} 
where $d_{njt}$ is a dummy for default status, $\boldsymbol{\beta}_n \in \mathbb{R}^{k \times 1}$ is a column vector of $k$ population coefficients randomized over countries, $\boldsymbol{x}_{njt-1} \in \mathbb{R}^{k \times 1}$ is a column vector of one-year lagged economic data corresponding to $k$ economic variables, $\boldsymbol{\alpha} \in \mathbb{R}^{\max(T_n) \times 1}$ is a column vector of the time-fixed effects to be estimated, and $\boldsymbol{\gamma}_t = \boldsymbol{1}(\tau = t)^{\max(T_n) \times 1}$ for each $\tau \in \{1, \dots, \max(T_n)\}$ is a dummy indicating year $t$, where $\boldsymbol{1}(\cdot)$ is the indicator function. Note that we are allowing the number of years $T_n$ vary according to country $n$, thus allowing an unbalanced panel to be estimated. Indeed, the mixed panel logit model is successful in handling unbalanced panels. However, all years in the data must be accounted for and assigned fixed effects, hence the usage of $\max(T_n)$ in the dimensions of $\boldsymbol{\alpha}$ and $\boldsymbol{\gamma}_t$.

$D_{njt}$ is a latent variable unobserved by the econometrician that is characterized by (1). $\varepsilon_{njt}$ is a random noise term that is also unobserved by the econometrician. We make the standard assumption of independent and identically distributed (i.i.d.) errors $\varepsilon_{njt}$ over countries $n$, years $t$, and default/non-default events $j$ according to a Type I extreme value distribution. Given that the errors are i.i.d. extreme value, we can ascribe a closed form to the probability of country $n$ experiencing a given sequence of default and non-default events for each year $t$ conditional on observing $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}_n$. In order to compress notation, we characterize a sequence of such events in a vector
\begin{gather*}
\boldsymbol{\mathit{d}}_n = [\mathit{d}_{n_{1}}, \dots, \mathit{d}_{n_{T_n}}]',
\end{gather*}
where the $\mathit{d_{n_t}}$'th element of $\boldsymbol{\mathit{d}}_n$ is a realization of $j$ for country $n$ in year $t$. That is, $\boldsymbol{\mathit{d}}_n$ is a sequence of 1s (defaults) and 0s (non-defaults) for a given country $n$ across all years for which data is available. Thus, the conditional probability of country $n$ experiencing an arbitrary sequence of events $\boldsymbol{\mathit{d}}_n$ is given by
\begin{align}
\mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n}(\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t | \boldsymbol{\alpha},\boldsymbol{\beta}) &= 
\mathlarger{\prod_{t=1}^{T_n}} \frac{e^{\boldsymbol{\beta}^{'} \boldsymbol{x}_{n  \mathit{d_{n_t}} t-1} + \boldsymbol{\alpha}^{'}\boldsymbol{\gamma}_t}}{\sum_{j=0}^1 e^{\boldsymbol{\beta}^{'} \boldsymbol{x}_{njt-1} + \boldsymbol{\alpha}^{'}\boldsymbol{\gamma}_t}} \nonumber \\
&= \mathlarger{\prod_{t=1}^{T_n}} \frac{e^{\boldsymbol{\beta}^{'} \boldsymbol{x}_{n  \mathit{d_{n_t}} t-1} + \boldsymbol{\alpha}^{'}\boldsymbol{\gamma}_t}}{e^{\boldsymbol{\beta}^{'} \boldsymbol{x}_{n0t-1} + \boldsymbol{\alpha}^{'}\boldsymbol{\gamma}_t} + e^{\boldsymbol{\beta}^{'} \boldsymbol{x}_{n1t-1} + \boldsymbol{\alpha}^{'}\boldsymbol{\gamma}_t}}.
\end{align}
This formula is similar to the well-known logit formula for i.i.d. extreme value errors, but instead includes the product of logits across years for a given country. Recall that we are able to multiply the logits in such a manner due to our independence assumption over $\varepsilon_{njt}$'s. Note that since the conditional probability is indexed by $n$, we have suppressed the subscript on $\boldsymbol{\beta}_n$ in (2). From here on, we denote the vector of coefficients as simply $\boldsymbol{\beta}$.

The econometrician, however, does not know $\boldsymbol{\beta}$ either. Thus, in order to obtain a probability expression which can be estimated given the data, we must integrate the probabilities conditional on $\boldsymbol{\beta}$ over its joint density $f(\boldsymbol{\beta}) \in \mathcal{C}^1$, which will generate the unconditional default probability:
\begin{gather*}
\mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n} = \int\limits_{supp(f)} \mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n}(\boldsymbol{\alpha},\boldsymbol{\beta}) f(\boldsymbol{\beta}) d\boldsymbol{\beta}.
\end{gather*}

The econometrician must also specify $f(\boldsymbol{\beta})$, which is known in the literature as the \textit{mixing distribution}. For sake of computational simplicity, we take $\boldsymbol{\beta}$ to be normally distributed about a vector of means $\boldsymbol{\mu} \in \mathbb{R}^{k \times 1}$ with covariance $\boldsymbol{\Sigma} \in \mathbb{R}^{k \times k}$; that is, $\boldsymbol{\beta} \sim \mathcal{N}_k(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. Thus the unconditional probability becomes 
\begin{align}
\mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n} = \int\limits_{supp(\Phi)} \mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n}(\boldsymbol{\alpha},\boldsymbol{\beta}) \mathlarger{\Phi}(\boldsymbol{\beta}|\boldsymbol{\mu}, \boldsymbol{\Sigma}) d\boldsymbol{\beta}.
\end{align}
which is now strictly a function of the parameters $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$.

\subsection{Simulation}

We simulate the unconditional probabilities generated by (3) using Monte Carlo sampling for \textit{each} coefficient $\beta_i$ in $\boldsymbol{\beta}$. In doing so, we assume that each $\beta_i$ is independently and normally distributed with a density parameterized by a mean $\mu_i$ and standard deviation $\sigma_i$; that is, 
\begin{gather*}
\beta_i \sim \mathcal{N}(\mu_i, \sigma_i).
\end{gather*}
We ultimately wish to estimate these parameters for all coefficients $i = 1, \dots, k$, which we now store in vectors $\boldsymbol{\mu} \in \mathbb{R}^{k \times 1}$ and $\boldsymbol{\sigma} \in \mathbb{R}^{k \times 1}$.

Using $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$, we take $R$ draws from each of the $k$ normal distributions and label each draw as $\boldsymbol{\beta}^{r}$. To simulate the unconditional probabilities we sum over the conditional probabilities $\mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n}(\boldsymbol{\beta}^{r}|\boldsymbol{\mu}, \boldsymbol{\sigma};\boldsymbol{\alpha})$ calculated for each draw $r$ using (2) and then normalized by the total number of draws $R$. The notation $\mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n}(\boldsymbol{\beta}^{r}|\boldsymbol{\mu}, \boldsymbol{\sigma}; \boldsymbol{\alpha})$ is meant to emphasize that each conditional probability expressed in (2) remains a function of the vector of year-fixed effects $\boldsymbol{\alpha}$ that has yet to be estimated as well as the randomized coefficients $\boldsymbol{\beta}$, which has now been parameterized in terms of its means $\boldsymbol{\mu}$ and standard deviations $\boldsymbol{\sigma}$ that have also yet to be estimated. \\

The simulated unconditional probability of default for country $n$ over $\boldsymbol{\mathit{d}}_n$ is thus
\begin{align}
\mathlarger{\hat{{\pi}}_{n\boldsymbol{\mathit{d}}_n}} &= 
\frac{1}{R} \sum\limits_{r=1}^{R} \mathlarger{\pi_{n\boldsymbol{\mathit{d}}_n}}(\boldsymbol{\beta}^{r}|\boldsymbol{\mu}, \boldsymbol{\sigma};\boldsymbol{\alpha}).
\end{align}

We can now proceed to estimate $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ via maximum likelihood estimation. We interpret the sum of logged simulated unconditional probabilities for default events across countries as the log-likelihood of $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ being the true population parameters \textit{conditional} on the sample data the econometrician observes. Abstracting away from training and test set considerations, given a dataset with $N$ countries where each country $n$ has an observed event sequence $\hat{\boldsymbol{\mathit{d}}_n}$, the \textit{net simulated log-likelihood function} is
\begin{align}
\mathlarger{\hat{\mathcal{L}}}(\boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t) &= \sum\limits_{n=1}^{N} \hat{\boldsymbol{\mathit{d}}_n}^{'} \ln\left(\mathlarger{\hat{\boldsymbol{\pi}}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}\right),
\end{align}
where we now decompress notation slightly for clarity and allow $\mathlarger{\hat{\boldsymbol{\pi}}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}$ to represent a $T_n \times 1$-vector containing simulated unconditional probabilities of default $\mathlarger{\hat{\pi}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}$ for country $n$, for \textit{each} year $t \in \{1, \dots, T_n\}$. (4) of course holds analogously with this time re-indexing, and each $\mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_{n_{t}}}$ is simply equal to the expression in (2), but without the product symbol.\\

Before we proceed with optimization of the simulated log-likelihood function over the entire training dataset, we implement supervised regularization over the parameters of interest in order to improve the out-of-sample performance of our calibrated model. Employing regularization over the parameters which characterize the distributions of the randomized coefficients $\boldsymbol{\beta}$ will lead to a decrease in the dispersion of the estimated distributions and thus reduce the variance of the estimated model in exchange for an increase in bias. $\lambda$, which is the weight placed on the regularization penalty term, will be chosen via $k$-fold cross validation. $\lambda^{CV}$ represents the optimal $\lambda$ chosen from the cross-validation selection algorithm and indicates the point of optimal trade-off between variance reduction and bias increase as measured by a loss function.

We implement L2 regularization over the parameters $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$. L2 regularization is commonly referred to as ridge regression when implemented in the context of ordinary least squares estimation. It penalizes increases in the magnitudes of the parameters according to the sum of their squares. Thus we augment the simulated log-likelihood function in (5) and define the \textit{penalized simulated log-likelihood function} $\hat{\mathcal{L}}_{pen}$, which is now also function of $\lambda$:
\begin{align}
\mathlarger{\hat{\mathcal{L}}}_{pen}(\lambda; \boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t) &= \sum\limits_{n=1}^{N} \hat{\boldsymbol{\mathit{d}}_n}^{'} \ln\left(\mathlarger{\hat{\boldsymbol{\pi}}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}\right) - \frac{\lambda}{2}\left(\boldsymbol{\alpha}^{'}\boldsymbol{\alpha} + \boldsymbol{\mu}^{'}\boldsymbol{\mu} + \boldsymbol{\sigma}^{'}\boldsymbol{\sigma}\right).
\end{align}

An added benefit of employing this form of regularization to our parameters of interest is that the penalty term $\frac{1}{2}\left(\boldsymbol{\alpha}^{'}\boldsymbol{\alpha} + \boldsymbol{\mu}^{'}\boldsymbol{\mu} + \boldsymbol{\sigma}^{'}\boldsymbol{\sigma}\right)$ can be interpreted as resulting from a standard normal distribution prior over the parameter matrix $\boldsymbol{\Omega}$ in a Bayesian \textit{maximum a posteriori} (MAP) estimation framework. To see this, consider $\boldsymbol{\Omega}$ to be a matrix of random variables consisting of the elements in $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ and then implement Bayes' theorem:
\begin{align}
\Pr\left(\boldsymbol{\Omega}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t\right) &= 
\frac{\Pr\left(\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t | \boldsymbol{\Omega}\right)\Pr\left(\boldsymbol{\Omega}\right)}{\Pr\left(\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t\right)}.
\end{align}
If we take our prior over the parameters in $\boldsymbol{\Omega}$ to be independent standard normal distributions, then the probability of $\boldsymbol{\Omega}$ is 
\begin{align*}
\Pr(\boldsymbol{\Omega}) &= \frac{1}{\sqrt{2\pi}}\prod_{ij} e^{-\frac{\omega_{ij}^2} {2}} & \forall ij \in \dim \boldsymbol{\Omega}.
\end{align*}
Since $\Pr\left(\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t\right)$ is fixed for the observed data and we have country panels that we must sum log-likelihoods over, taking logs of (7) gives us the net MAP log-likelihood
\begin{align*}
\mathlarger{\mathcal{L}}_{MAP}\left(\boldsymbol{\Omega}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t\right) &= \ln \Pr\left(\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t | \boldsymbol{\Omega}\right) + \ln \Pr(\boldsymbol{\Omega}) \\
&= \sum\limits_{n=1}^{N} \hat{\boldsymbol{\mathit{d}}_n}^{'} \ln\mathlarger{\boldsymbol{\pi}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}(\boldsymbol{\beta}^{r}|\boldsymbol{\Omega}) - \frac{1}{2} \sum\limits_{ij} \omega_{ij}^2 + \ln \left(\frac{1}{\sqrt{2\pi}}\right)\\
&= \sum\limits_{n=1}^{N} \hat{\boldsymbol{\mathit{d}}_n}^{'} \ln\mathlarger{\boldsymbol{\pi}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}(\boldsymbol{\beta}^{r}|\boldsymbol{\Omega}) - \frac{1}{2}\left(\boldsymbol{\alpha}^{'}\boldsymbol{\alpha} + \boldsymbol{\mu}^{'}\boldsymbol{\mu} + \boldsymbol{\sigma}^{'}\boldsymbol{\sigma}\right) + \ln \left(\frac{1}{\sqrt{2\pi}}\right),
\end{align*}
which is precisely the non-simulated version of (6) with $\lambda = 1$ and an additional immaterial constant term $\ln \left(\frac{1}{\sqrt 2 \pi}\right)$, re-parameterizing $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ as $\boldsymbol{\Omega}$.

\subsection{Cross-validation Selection of L2 Regularization Parameter}

We now describe the cross-validation selection algorithm that chooses $\lambda^{CV}$. We begin by randomly partitioning our entire set of training data $X$ into $K$ disjoint folds of (roughly) equal size indexed by $k$ such that $\cup_{k=1}^{K} X_{k} = X$ and $\cap_{k=1}^{K} X_{k} = \varnothing$. Then we decide on a range, or \textit{grid}, of candidate $\lambda$'s to iterate over. We thus pull each candidate $\lambda_i \in \{0, 1, 2, \dots, L-1, L\}$, where $\lambda_i \in \mathbb{Z}_{\geq 0}$ and $L$ is sufficiently large.\\

For \textit{each} $\lambda_i$, we run the following procedure:
\begin{enumerate}[listparindent=\parindent]
\item For each $k = 1, \dots, K$, we estimate the parameters $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ by maximizing the penalized simulated log-likelihood function in (6), but using only the training data in the remaining $K-1$ folds. The penalized simulated log-likelihood function is thus
\begin{align}
\mathlarger{\hat{\mathcal{L}}}_{pen}(\lambda_i; \boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t) &= \mathlarger{\sum}\limits_{n \in X \setminus X_k} \hat{\boldsymbol{\mathit{d}}_n}^{'} \ln\left(\mathlarger{\hat{\boldsymbol{\pi}}}_{n\boldsymbol{\mathit{d}}_{n_{t}}}\right) - \frac{\lambda_i}{2}\left(\boldsymbol{\alpha}^{'}\boldsymbol{\alpha} + \boldsymbol{\mu}^{'}\boldsymbol{\mu} + \boldsymbol{\sigma}^{'}\boldsymbol{\sigma}\right),
\end{align}
where the summation index acknowledges that the $K-1$ folds may contain only a proper subset of the $N$ countries in the training data.

Functions with the form of $\mathlarger{\hat{\mathcal{L}}}_{pen}$ are typically optimized numerically. We maximize $\mathlarger{\hat{\mathcal{L}}}_{pen}$ with respect to $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ using the Nelder-Mead algorithm, with the assistance of the \textit{pylogit} package in Python. We defer a description of the Nelder-Mead algorithm to the Data Appendix. Using (8), the optimal $\hat{\boldsymbol{\alpha}}^{k}, \hat{\boldsymbol{\mu}}^{k}, \hat{\boldsymbol{\sigma}}^{k}$ for fold $k$ are
\begin{align*}
\{\hat{\boldsymbol{\alpha}}^{k}, \hat{\boldsymbol{\mu}}^{k}, \hat{\boldsymbol{\sigma}}^{k}\} &= \underset{\{\boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}\}}{\arg \max} \ \ \mathlarger{\hat{\mathcal{L}}}_{pen}(\lambda_i; \boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t).
\end{align*}

\item We take $\hat{\boldsymbol{\alpha}}^{k}, \hat{\boldsymbol{\mu}}^{k}, \hat{\boldsymbol{\sigma}}^{k}$, which were estimated using the data $X \setminus X_k$, and now back out the predicted probabilities using the data in fold $k$ which was excluded in Step 1. In order to do so, we must re-simulate the unconditional probabilities of default as in (4), but this time by drawing values for the coefficients from normal distributions with means and standard deviations that have been estimated as $\hat{\boldsymbol{\mu}}^{k}$ and $\hat{\boldsymbol{\sigma}}^{k}$; that is,
\begin{align*}
\hat{\beta}_{i}^{k} \sim \mathcal{N}(\hat{\mu}_i^{k}, \hat{\sigma}_i^{k}),
\end{align*}
for $i = 1, \dots, k$, where this $k$ now refers to the total numbers of coefficients and \textbf{not} the index for the current fold.

As in our initial simulation, we take $R$ draws from each normal distribution and label the $r$'th draw of coefficients as $\hat{\boldsymbol{\beta}}^{k,r}$. The simulated unconditional probability is again the sum of conditional probabilities $\mathlarger{\pi^{k}}_{n\boldsymbol{\mathit{d}}_n}(\hat{\boldsymbol{\beta}}^{k,r}|\hat{\boldsymbol{\mu}}^{k}, \hat{\boldsymbol{\sigma}}^{k}; \hat{\boldsymbol{\alpha}}^{k})$ evaluated for each draw $r$ using data $X_k$, normalized by $R$:
\begin{align}
\mathlarger{\hat{\pi}^{k}}_{n\boldsymbol{\mathit{d}}_n} &= 
\frac{1}{R} \sum\limits_{r=1}^{R} \mathlarger{\pi^{k}}_{n\boldsymbol{\mathit{d}}_n}(\hat{\boldsymbol{\beta}}^{k,r}|\hat{\boldsymbol{\mu}}^{k}, \hat{\boldsymbol{\sigma}}^{k}; \hat{\boldsymbol{\alpha}}^{k}).
\end{align}
Re-indexing the probabilities in (9) by $t$ and storing each unconditional probability of default for country $n$ in year $t$ present in $X_k$, we obtain the vector of probabilities $\mathlarger{\hat{\boldsymbol{\pi}}}^{k}_{n\boldsymbol{\mathit{d}}_{n_{t}}}$ that is present in the penalized simulated log-likelihood function.

\item We evaluate the fitted model's performance on the validation set $X_k$ in order to estimate the cross-validation error \textit{for fold} $k$. We specify the loss function to be the \textit{negative} penalized likelihood, which is basically the negation of (8) evaluated at $\hat{\boldsymbol{\alpha}}^{k}, \hat{\boldsymbol{\mu}}^{k}, \hat{\boldsymbol{\sigma}}^{k}$, but instead only summed over the countries $n$ that happen to be present in $X_k$:
\begin{align*}
NLL^{k}(\lambda_i) &= -\left(\mathlarger{\sum}\limits_{n \in X_k} \hat{\boldsymbol{\mathit{d}}_n}^{'} \ln\left(\mathlarger{\hat{\boldsymbol{\pi}}}^{k}_{n\boldsymbol{\mathit{d}}_{n_{t}}}\right) - \frac{\lambda_i}{2}\left(\hat{\boldsymbol{\alpha}}^{k^{\bm{'}}}\hat{\boldsymbol{\alpha}}^{k} + \hat{\boldsymbol{\mu}}^{k^{\bm{'}}}\hat{\boldsymbol{\mu}}^{k} + \hat{\boldsymbol{\sigma}}^{k^{\bm{'}}}\hat{\boldsymbol{\sigma}}^{k}\right)\right)
\end{align*}

\item Steps 1, 2, and 3 are iterated over each fold $k$. In order to calculate the model's overall cross-validation error given $\lambda_i$, we average the cross-validation errors $NLL^{k}(\lambda_i)$ for each fold $k$, which is 
\begin{align*}
NLL(\lambda_i) &= \frac{1}{K} \sum \limits_{k=1}^{K} NLL^{k}(\lambda_i).
\end{align*}
\end{enumerate} 

The negative log-likelihoods $NLL(\lambda_i)$ generated by this procedure are stored. The chosen $\lambda^{CV}$ is the $\lambda_{i}$ with the \textbf{smallest} corresponding negative log-likelihood.

\subsection{Prediction of Unconditional Default Probabilities}

After cross-validation, we are now able to predict unconditional probabilities of default in our test dataset $Z$, which represents data that has occurred after the estimation procedure; that is, data spanning some number of years $t > \max(T_n)$. We begin by fitting (8) on the \textit{entire} training dataset $X$ using the optimal $\lambda_i = \lambda^{CV}$ and estimating $\boldsymbol{\alpha}, \boldsymbol{\mu},$ and $\boldsymbol{\sigma}$ using Nelder-Mead:
\begin{align*}
\{\boldsymbol{\alpha}^*, \boldsymbol{\mu}^*, \boldsymbol{\sigma}^*\} &= \underset{\{\boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}\}}{\arg \max} \ \ \mathlarger{\hat{\mathcal{L}}}_{pen}(\lambda^{CV}; \boldsymbol{\alpha}, \boldsymbol{\mu}, \boldsymbol{\sigma}|\boldsymbol{x}_{njt-1}, \boldsymbol{\gamma}_t).
\end{align*}

Predicted unconditional probabilities for country $n$ are thus
\begin{align}
\mathlarger{\pi^{*}}_{n\boldsymbol{\mathit{d}}_n} &= 
\frac{1}{R} \sum\limits_{r=1}^{R} \mathlarger{\pi}_{n\boldsymbol{\mathit{d}}_n}(\boldsymbol{\beta}^{*,r}|\boldsymbol{\mu}^{*}, \boldsymbol{\sigma}^{*}; \boldsymbol{\alpha}^{*}).
\end{align}

\section{Empirical Analysis} \label{sec:empirics}

\subsection{Data}

In order to estimate the model described in section \ref{sec:theory}, we use data on external debt defaults from Reinhart \& Rogoff. Economic data for various countries were obtained from. We could not find key data for some of the more industrialized countries, thus they were omitted from our sample. The list of all countries present in our sample as well as the range of years for which we were able to obtain data for each country is given as Table A.1 in the Appendix.

Because we are interested in the applicability of our model to perform forward-looking prediction, we use time as the cutoff criteria for dividing our entire dataset into a training set and a test set. Specifically, we bin pre-2010 observations into our training set and post-2009 observations into our test set. 2010 represents a natural temporal divide for that data as default histories are relatively quiet during the 2000s but jump in the early 2010s. We would like to measure how well our model captures this spike in default activity using historical data while controlling for year-specific influences.

Partioning the data in such a manner implies 683 observations in the training set spanning 60 countries and 16 years. The test set contains 268 observations spanning 60 countries and 6 years. 

The table below reports summary statistics of the $21$ lagged economic variables over the training set and the test set separately.

\subsection{Cross-validation Outcome}

We refer to subsection 2.4 for a discussion of how we perform $k$-fold cross-validation to estimate the optimal $\lambda^{CV}$. In our empirical analysis, we let $k=10$ and perform 10-fold cross-validation. The graph below displays the cross-validated negative log-likelihoods against the log of their corresponding $\lambda$'s obtained from the validation algorithm. The U-shaped nature of the curve is indicative of the established bias-variance trade-off present in regularization. \\

The optimal $\lambda^{CV}$ is 

\subsection{Mixed Panel Logit Estimation Output}

The table below reports the estimated year-fixed effects for each year present in the sample and the estimated means and standard deviations of the randomized coefficients for our lagged economic variables of interest.

\subsection{Predicted Probabilities and Out-of-Sample Performance}

The table below lists the predicted probabilities of default for the top-10 likeliest countries to default for each year post-2009 present in the test set. \\

In order to decide a cutoff value for logits that would predict which events are defaults and which are non-defaults, we . Using this cutoff metric, we can now measure out-of-sample performance using the \textit{classification rate}.

\section{Discussion} \label{sec:discussion}

Our results show \\

We would like to compare the model's out-of-sample performance with that of long term bond ratings issues by credit rating agencies in late 2009. This would imply performing an ROC curve analysis. However, we did not have enough space to fit that analysis in this paper. Undertaking such an analysis is one avenue of further research. The next avenue we wish to take

\section{Conclusion} \label{sec:conclusion}

In conclusion, 



\singlespacing
\setlength\bibsep{0pt}
\bibliographystyle{my-style}
\bibliography{Placeholder}


\clearpage

\onehalfspacing

%\section*{Tables} \label{sec:tab}
%\addcontentsline{toc}{section}{Tables}



\clearpage

%\section*{Figures} \label{sec:fig}
%\addcontentsline{toc}{section}{Figures}

%\begin{figure}[hp]
%  \centering
%  \includegraphics[width=.6\textwidth]{../fig/placeholder.pdf}
%  \caption{Placeholder}
%  \label{fig:placeholder}
%\end{figure}



\clearpage


\appendix
\section*{Appendix}


\newpage

\section*{Data Appendix} \label{sec:data appendix}
\addcontentsline{toc}{section}{Appendix}

Code and data is available at \url{https://github.com/kenrios1993/mixed_panel_logit_default_risk}. \\
We use the \textit{pylogit} package in Python extensively for simulation and optimization using Nelder-Mead. Source files and documentation for \textit{pylogit} are available at \url{https://github.com/timothyb0912/pylogit}.

\subsection*{Nelder-Mead Algorithm}

\end{document}
